{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le web scraping (parfois appelé harvesting) est une technique d'extraction du contenu de sites Web, via un script\n",
    "ou un programme, dans le but de le transformer pour permettre son utilisation dans un autre contexte, par exemple\n",
    "le référencement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le web scraping peut être utilisé pour récupérer des métadonnées. C'est ainsi que la bibliothèque multimédia \n",
    "XBMC récupère les informations sur les médias qu'elle gère (affiches et résumés des films et séries télévisées, \n",
    "jaquette des albums…) sur différents sites tels que IMDb ou AlloCiné au moyen de scrapers dédiés.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il y plusieurs types de librairie | framework qui nous permettent de faire du WebScraping. \n",
    "Notamment Scrapy, Selenium, BeautifulSoup, pour ne citer que les plus connus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scrapy est un framework open-source permettant la création de robots d'indexation. Développé en Python, il \n",
    "dispose d'une forte communauté, offrant de nombreux modules supplémentaires. La première version stable a été \n",
    "publiée en septembre 2009. Depuis, l'équipe de développement publie régulièrement de nouvelles versions dans le \n",
    "but d'enrichir le framework en fonctionnalité. L'objectif principal est d'obtenir une API stable pour la version\n",
    "1.02. Le framework dispose d'une communauté active, et un support commercial est effectué par plusieurs entreprises.\n",
    "\n",
    "Le framework est compatible Python 2.7 et Python 3.6 ou au-dessus, sur la majorité des plates-formes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caractéristiques de Scrapy**\n",
    "\n",
    "L'équipe responsable du développement du framework lui confère plusieurs caractéristiques :\n",
    "\n",
    "    1. Simple : aucune notion avancée en Python n'est nécessaire pour utiliser Scrapy\n",
    "    2. Productif : l'empreinte de code à générer est très courte, la plupart des opérations sont gérées par Scrapy\n",
    "    3. Rapide : le framework est rapide, avec une gestion d'actions en parallèle notamment\n",
    "    4. Extensible : chaque robot peut être personnalisés via des extensions, modifiant son comportement\n",
    "    5. Portable : les robots Scrapy sont compatibles Linux, Windows, Mac et BSD\n",
    "    6. Open Source\n",
    "    7. Robuste, grâce à une batterie de tests effectuées aussi bien par les développeurs que la communauté\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour installer scrapy : pip install scrapy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Avec conda : conda install -c conda-forge scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation de scrapy :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.scrapy.org/en/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour connaître les différentes étapes de la création d'un jeu de données avec scrapy.\n",
    "\n",
    "https://ledatascientist.com/creer-un-jeu-de-donnees-avec-scrapy/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemple de Robot:\n",
    "\n",
    "Le site web du projet propose un tutoriel détaillé sur l'utilisation de Scrapy5. Ce cours propose notamment \n",
    "plusieurs exemples, comme celui ci-contre, extrayant certains liens présents sur plusieurs pages web.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.spider import BaseSpider\n",
    "from scrapy.selector import HtmlXPathSelector\n",
    "\n",
    "from tutorial.items import DmozItem\n",
    "\n",
    "class DmozSpider(BaseSpider):\n",
    "   name = \"dmoz\"\n",
    "   allowed_domains = [\"dmoz.org\"]\n",
    "   start_urls = [\n",
    "       \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n",
    "       \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n",
    "   ]\n",
    "\n",
    "   def parse(self, response):\n",
    "       hxs = HtmlXPathSelector(response)\n",
    "       sites = hxs.select('//ul/li')\n",
    "       items = []\n",
    "       for site in sites:\n",
    "           item = DmozItem()\n",
    "           item['title'] = site.select('a/text()').extract()\n",
    "           item['link'] = site.select('a/@href').extract()\n",
    "           item['desc'] = site.select('text()').extract()\n",
    "           items.append(item)\n",
    "       return items\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finalisation du Spider\n",
    "\n",
    "La dernière étape pour avoir de la donnée est de retourner des items. Cette étape est essentielle. \n",
    "Et elle consiste juste à créer une instance de ReviewsAllocineItem à chaque commentaire, à renseigner chacun de \n",
    "ses champs avec les valeurs scrapées puis les “yield”. Notre fichier spider final à ça :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-9ae605ceddd8>, line 31)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-3-9ae605ceddd8>\"\u001b[1;36m, line \u001b[1;32m31\u001b[0m\n\u001b[1;33m    Pour lancer notre spider et avoir les données scrapées dans un fichier CSV, on fait la commande suivante :\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from scrapy import Request, Spider\n",
    "from ..items import ReviewsAllocineItem\n",
    " \n",
    "class SpiderReviewsAllocine(Spider):\n",
    "    # Nom du spider\n",
    "    name = \"reviews_allocine\"\n",
    "    # URL de la page à scraper\n",
    "    url = \"https://www.allocine.fr/film/fichefilm-10568/critiques/spectateurs/\"\n",
    " \n",
    "    def start_requests(self):\n",
    "        yield Request(url=url, callback=self.parse_films)\n",
    " \n",
    "    def parse_films(self, response):\n",
    "        title = response.css(\"a.titlebar-link::text\").extract_first()\n",
    "        review_blocks = response.css(\"div.review-card\")\n",
    "        for review_card in review_blocks:\n",
    "            review = review_card.css(\"div.content-txt::text\").extract_first()\n",
    "            stars = review_card.css(\"span.stareval-note::text\").extract_first()\n",
    "             \n",
    "            # Pour avoir la note (nombre d'étoiles) en float\n",
    "            stars = float(stars.replace(\",\", \".\"))\n",
    " \n",
    "            item = ReviewsAllocineItem()\n",
    " \n",
    "            item['title'] = title\n",
    "            item['stars'] = stars\n",
    "            item['review'] = review\n",
    " \n",
    "            yield item\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour lancer notre spider et avoir les données scrapées dans un fichier CSV, on fait la commande suivante :\n",
    "scrapy crawl reviews_allocine -o reviews_allocine.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources utiles pour la création d'un jeu de données avec scrapy\n",
    "\n",
    "https://medium.com/@alexandrewrg/scraper-les-donn%C3%A9es-de-plusieurs-pages-avec-scrapy-2e076ac7dc09\n",
    "\n",
    "\n",
    "https://ledatascientist.com/creer-un-jeu-de-donnees-avec-scrapy/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
