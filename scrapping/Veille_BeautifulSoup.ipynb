{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Définition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beautiful Soup** est une bibliothèque Python permettant d'extraire des données de fichiers HTML et XML. Il fonctionne avec votre analyseur préféré pour fournir des moyens idiomatiques de naviguer, de rechercher et de modifier l'arborescence d'analyse. Cela permet généralement aux programmeurs d'économiser des heures ou des jours de travail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation de BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (4.9.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exemple d'utilisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contexte "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons le site de *seloger.com*. On veut chercher des annonces qui correspondent à un loyer de 1200€ max, sur Paris, ... . Le problème c'est qu'il y a près de 6000 annonces proposées sur 300 pages, chaque page regroupant 20 annonces.\n",
    "Voici les 3 premières pages :\n",
    "\n",
    "```\n",
    "https://www.seloger.com/immobilier/locations/immo-paris-75/bien-appartement/?LISTING-LISTpg=1\n",
    "https://www.seloger.com/immobilier/locations/immo-paris-75/bien-appartement/?LISTING-LISTpg=2\n",
    "https://www.seloger.com/immobilier/locations/immo-paris-75/bien-appartement/?LISTING-LISTpg=3\n",
    "```\n",
    "\n",
    "Elles sont quasi identiques. Du coup pour obtenir la liste de ces 300 pages, il suffit de changer le dernier chiffre qui indique le numéro de la page. On peut le faire de la façon suivante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = 'https://www.seloger.com/immobilier/locations/immo-paris-75/bien-appartement/?LISTING-LISTpg='\n",
    "\n",
    "def get_pages(token, nb):\n",
    "    pages = []\n",
    "    for i in range(1,nb+1):\n",
    "        j = token + str(i)\n",
    "        pages.append(j)\n",
    "    return pages\n",
    "\n",
    "\n",
    "pages = get_pages(token,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Navigation dans un document HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que l'on a une liste d'adresse web, il faut que l'on puisse y accéder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (from requests) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (from requests) (1.26.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\utilisateur\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n"
     ]
    }
   ],
   "source": [
    "# requests est une librairie HTTP python qui a pour but de simplifier les requêtes HTTP\n",
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "for i in pages:\n",
    "    response = requests.get(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A chaque itération, la fonction ```requests.get()``` essaie de se connecter à une adresse dans notre liste et enregistre résultat dans la variable ```response```. Si la tentative est réussie, la variable ```response``` contiendra la valeur ```<Response [200]>``` ainsi que le code source de la page à laquelle on souhaite accéder. Utilisez ```response.text``` pour l’afficher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les pages web sont écrites en HTML. Le code HTML contient des balises qui aident les navigateurs à afficher correctement le contenu de la page. Le plus souvent les informations que nous souhaitons extraire se trouvent au milieu des balises. Voici un exemple :\n",
    "\n",
    "```\n",
    "<!DOCTYPE html>  \n",
    "<html>  \n",
    "    <head>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1> Jolie studio dans le 10ème</h1>\n",
    "        <p class=\"promo\"> 1100 euros charges comprises</p>\n",
    "        <p> Tel: 06 82 23 21 </p> \n",
    "    <body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "Le titre de l’annonce se trouve entre les balises ```<h1>```…```</h1>```. Le prix de l’appartement et le numéro de téléphone de l’agence se situent au milieu des balises ```<p>```…```</p>```. Parfois les balises peuvent avoir une ```class``` particulière, une sorte de nom propre qui les distingue des autres balises du même type.\n",
    "\n",
    "Le code HTML du site seloger.com est beaucoup plus complexe. On peut le visualiser à l’aide du navigateur Chrome en faisant un clique droit sur la page affichée et en choisissant “Inspecter”. La pluspart des navigateurs sont dotés d’une fonctionnalité qui permet la recherche des balises dans le code de la page à partir de mots clés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s’avère que les informations que nous voulons extraire se trouvent entre les balises ```<em>```...```</em>``` :\n",
    "\n",
    "```\n",
    "<em class=\"agency-website\" data-codeinsee=\"750118\" data-codepostal=\"75018\" data-idagence=\"46600\" data-idannonce=\"119026673\" data-idtiers=\"22739\" data-idtypepublicationsourcecouplage=\"SL\" data-nb_chambres=\"0\" data-nb_photos=\"6\" data-nb_pieces=\"1\" data-position=\"3\" data-prix=\"999 €\" data-produitsvisibilite=\"AD:AC:BB:BX:AW\" data-surface=\"32,2400016784668\" data-typebien=\"1\" data-typedetransaction=\"1\"> Site web </em>\n",
    "```\n",
    "\n",
    "Pour accéder aux informations de cette balise, il faudra la retrouver dans le code HTML de notre variable ```response```. On peut le faire à l’aide de la librairie BeautifulSoup. Nous transformons d’abord ```response``` en objet BeautifulSoup et cherchons ensuite toutes les balises ```<em>```...```</em>``` à l’aide de la fonction ```find_all()```. On précise entre parenthèses que nous voulons les balises ```<em>```...```</em>``` qui portent le nom ```class=\"agency-website\"``` :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "\n",
    "soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "em_box = soup.find_all(\"em\", {\"class\":\"agency-website\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons presque atteint notre objectif ! Il nous reste à extraire une par une les informations qui nous intéressent. La variable ```em_box``` fonctionne comme un dictionnaire. Quand on donne une clé à un de ses éléments, par exemple, ```em_box[3]['data-prix']```, elle nous renvoie sa valeur: ```'999 €'```. Nous pouvons donc faire une boucle qui extrait toutes les valeurs qu’on désire et les range dans une table :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "parameters = ['data-prix','data-codepostal','data-idagence','data-idannonce','data-nb_chambres','data-nb_pieces','data-surface','data-typebien']\n",
    "\n",
    "df_f = pd.DataFrame()\n",
    "\n",
    "for par in parameters:\n",
    "    l = []\n",
    "    \n",
    "    for el in em_box:\n",
    "        j = el[par]\n",
    "        l.append(j)\n",
    "        \n",
    "    l = pd.DataFrame(l, columns = [par])\n",
    "    \n",
    "    df_f = pd.concat([df_f,l], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les deux dernières lignes de code regroupent les valeurs extraites en colonnes et joignent ces colonnes ensemble. Et voilà, nous avons réussi à scraper une page :\n",
    "\n",
    "![r4g6ire9.bmp](https://miro.medium.com/max/2400/1*5_ogdVz7zTlrhboICW9AsA.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ne pas passer pour un robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant il faut répéter ça pour les 299 autres pages. Le problème c'est qu'au bout de quelques itérations la fonction ```requests.get()``` renverra la variable ```response``` suivante :\n",
    "\n",
    "```\n",
    "<!DOCTYPE html>\n",
    "\n",
    "<html>\n",
    "<head>\n",
    "<meta content=\"noindex,nofollow\" name=\"robots\"/>\n",
    "\n",
    "```\n",
    "\n",
    "La balise ```meta``` montre que seloger.com nous a identifié comme un robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour connaître la liste des robots autorisés sur un site, il faut lire son fichier ```robots.txt```:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "with urlopen(\"https://www.seloger.com/robots.txt\") as stream:\n",
    "    print(stream.read().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un extrait de ```robots.txt``` du site seloger.com :\n",
    "![image_robots.txt](https://miro.medium.com/max/2400/1*xI7MNnWIa1Z3rozhJsQ_Sg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On constate qu’il bannit les robots comme *Srapy*, *Zao*, *UbiCrawler*, *Nutch* et bien d’autres. Le nom du robot est précédé par ```User_agent```. C’est une sorte de carte d’identité de l’utilisateur collectée par les sites. Les robots se présentent simplement avec leur noms. Cependant, si on accède à seloger.com depuis un navigateur, le champ ```User_agent``` contiendra le nom de notre navigateur et les informations concernant notre système d’exploitation, par exemple : ```Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:52.0) Gecko/20100101 Firefox/52.0```.\n",
    "\n",
    "Quel est l’```User_agent``` de notre robot ? Essayez la commande suivante: ```requests.utils.default_user_agent()```. En l’exécutant vous allez apprendre que votre robot s’appelle ‘python-requests/2.21.0’. Il ne figure pas sur la liste des mauvais robots, mais cela ne signifie pas pour autant qu’il est le bienvenu. Le serveur de seloger.com détecte rapidement son comportement ‘robotique’ et le bloque temporairement.\n",
    "\n",
    "À la différence des humains, les robots :\n",
    "\n",
    "* N’utilisent pas les navigateurs et possèdent donc un mauvais ```User_agent```\n",
    "* Restent très peu de temps sur une page\n",
    "* Demandent plusieurs pages à la fois depuis la même adresse IP\n",
    "\n",
    "Afin d’éviter d’être bloqué, on peut demander à notre robot de se présenter correctement, comme un humain, en précisant un bon ```User_agent```. Pour ne pas endommager le site et se comporter de façon plus “humaine”, on peut également demander à notre robot de patienter quelques secondes sur une page avant de passer à une autre. Enfin, à chaque passage, nous pouvons changer l’adresse IP du robot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici comment réaliser ces ajustements. Commençons par créer un pool de proxies. On trouve plusieurs listes de proxies gratuits sur Internet (exemple: www.proxy-list.download/HTTPS). Il faudra la télécharger et l’importer l'espace de travail. On crée ensuite un **itérateur**, un objet qui nous permettra changer de proxy à chaque connexion à l’aide de la fonction ```next()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "proxies = pd.read_csv('proxy_list.txt', header = None)\n",
    "proxies = proxies.values.tolist()\n",
    "proxies = list(it.chain.from_iterable(proxies))\n",
    "proxy_pool = it.cycle(proxies)\n",
    "proxy = next(proxy_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, apprenons notre robot à se présenter correctement et à être moins impatient. Pour le faire, nous utilisons les librairies ```time```,```random``` et ```fake-useragent```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "\n",
    "# !pip install fake-useragent\n",
    "\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "ua = UserAgent()\n",
    "time.sleep(random.randrange(1,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Par conséquent, on modifie notre fonction ```requests.get()```:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(i,proxies={\"http\": proxy, \"https\": proxy}, headers={'User-Agent': ua.random},timeout=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elle contient maintenant un ```proxy``` qui change à chaque nouvelle exécution de la ligne ```proxy = next(proxy_pool)``` et une fausse identité ```ua.random```. Nous avons ajouté également l’argument ```timeout=5``` qui signifie que chaque tentative de connexion à une page dure 5 secondes maximum.\n",
    "\n",
    "Les proxies gratuits ne sont jamais très fiables. Afin de revenir sur une page que notre fonction ```requests.get()``` n’est pas parvenue à ouvrir à cause d’une mauvaise connexion, nous avons ajouté la condition suivante :\n",
    "\n",
    "```\n",
    "while len(pages) > 0:\n",
    "    try:\n",
    "       ...\n",
    "       pages.remove(i)\n",
    "    except:\n",
    "       print(\"Skipping. Connnection error\")\n",
    "```\n",
    "\n",
    "La boucle ```while``` répétera une tentative de connexion jusqu’à l’épuisement des pages à consulter dans notre liste.\n",
    "\n",
    "Avec une telle requête, le serveur ne saura jamais que vous êtes un robot et vous obtiendriez toutes les informations souhaitées. Notons toutefois que seloger.com interdit le scraping dans ses conditions d’utilisation. L’exécution du code de ce tutoriel relève donc entièrement de votre responsabilité !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les techniques de scraping facilitent la collecte et le traitement des données dispersées sur intternet. Pour le réaliser il faut:\n",
    "\n",
    "* Créer une liste des pages web à parcourir.\n",
    "* Localiser l’information qui vous intéresse dans le code source de ces pages. Avoir des bases en HTML vous facilitera cette étape du travail.\n",
    "* Une fois que vous avez identifié les balises à extraire, vous pouvez créer une boucle qui répète la même opération pour plusieurs pages.\n",
    "* Si le site vous bloque, changez d’User_agent et utilisez un proxy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code complet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import bs4\n",
    "import random\n",
    "import requests\n",
    "# !pip install fake-useragent\n",
    "from fake_useragent import UserAgent\n",
    "import itertools as it\n",
    "\n",
    "token = 'https://www.seloger.com/immobilier/locations/immo-paris-75/bien-appartement/?LISTING-LISTpg='\n",
    "\n",
    "def get_pages(token, nb):\n",
    "    pages = []\n",
    "    for i in range(1,nb+1):\n",
    "        j = token + str(i)\n",
    "        pages.append(j)\n",
    "    return pages\n",
    "\n",
    "pages = get_pages(token,295)\n",
    "\n",
    "# https://www.proxy-list.download/HTTPS\n",
    "proxies = pd.read_csv('proxy_list.txt', header = None)\n",
    "proxies = proxies.values.tolist()\n",
    "proxies = list(it.chain.from_iterable(proxies))\n",
    "\n",
    "def get_data(pages,proxies):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    parameters = ['data-prix','data-codepostal','data-idagence','data-idannonce','data-nb_chambres','data-nb_pieces','data-surface','data-typebien']\n",
    "    ua = UserAgent()\n",
    "    proxy_pool = it.cycle(proxies)\n",
    "    \n",
    "    while len(pages) > 0:\n",
    "        for i in pages:\n",
    "        # on lit les pages une par une et on initialise une table vide pour ranger les données d'une page     \n",
    "            df_f = pd.DataFrame()\n",
    "        # itération dans un liste de proxies    \n",
    "            proxy = next(proxy_pool)\n",
    "        # essai d'ouverture d'une page   \n",
    "            try:\n",
    "                response = requests.get(i,proxies={\"http\": proxy, \"https\": proxy}, headers={'User-Agent': ua.random},timeout=5)\n",
    "                time.sleep(random.randrange(1,5))\n",
    "        # lecture du code html et la recherche des balises <em>\n",
    "                soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "                em_box = soup.find_all(\"em\", {\"class\":\"agency-website\"})\n",
    "        # extraction des données        \n",
    "                for par in parameters:\n",
    "                    l = []\n",
    "                    for el in em_box:\n",
    "                        j = el[par]\n",
    "                        l.append(j)\n",
    "                    l = pd.DataFrame(l, columns = [par])\n",
    "                    df_f = pd.concat([df_f,l], axis = 1)\n",
    "                df = df.append(df_f, ignore_index=True)\n",
    "                pages.remove(i)\n",
    "                print(df.shape)\n",
    "            except:\n",
    "                print(\"Skipping. Connnection error\")\n",
    "                \n",
    "    return df\n",
    "\n",
    "data = get_data(pages,proxies)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
